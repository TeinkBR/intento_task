{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOV1RWUGPmYwOJKI+oKWoye",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TeinkBR/intento_task/blob/master/additional_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas --quiet\n",
        "!pip install nltk --quiet\n",
        "!pip install tmx2dataframe --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8zV93PWYcZC",
        "outputId": "44f375f6-4a8b-4135-efad-04dc014b17ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tmx2dataframe (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xml --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg_SgtSqaJDQ",
        "outputId": "4ed8be8f-1412-42bb-dc80-8559dbe827a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement xml (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for xml\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "7XkTi4GTYW6e",
        "outputId": "2565ad40-5d7d-407c-e8a5-7812c527215b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nif __name__ == \\'__main__\\':\\n    file_names = [\\'hr-it.tmx\\', \\'es-is.tmx\\', \\'en-pl.tmx\\']\\n    cleaned_file_names = [\\'Cleaned_hr-it_dataset.csv\\', \\'Cleaned_es-is_dataset.csv\\', \\'Cleaned_en-pl_dataset.csv\\']\\n\\n    for i, file_name in enumerate(file_names):\\n        dataset_cleaner = DatasetCleaner([file_name])\\n        dataset_cleaner.clean_dataset()\\n        dataset_cleaner.df_filtered.sort_values(by=[\\'source_symbol_count\\'], ascending=True, inplace=True)\\n        summary = dataset_cleaner.summarize()\\n        print(f\"Summary for {file_name}: {summary}\")\\n        dataset_cleaner.df_filtered.to_csv(cleaned_file_names[i], index=False)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from tmx2dataframe import tmx2dataframe\n",
        "import xml.parsers.expat\n",
        "class DatasetCleaner:\n",
        "    \"\"\"\n",
        "    A class for cleaning and filtering bilingual datasets stored in TMX files.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, file_name):\n",
        "        \"\"\"\n",
        "        Initialize the DatasetCleaner object with the given TMX file name.\n",
        "        \"\"\"\n",
        "        self.file_name = file_name\n",
        "        self.df = self.read_tmx_file()\n",
        "    \n",
        "    def read_tmx_file(self):\n",
        "      \"\"\"\n",
        "      Read the TMX file and return a Pandas DataFrame.\n",
        "      \"\"\"\n",
        "      metadata, df = tmx2dataframe.read(self.file_name)\n",
        "      df = df.rename(columns={\"source_sentence\": \"source\", \"target_sentence\": \"target\"})\n",
        "      df['inner_id'] = range(1, len(df) + 1)\n",
        "      return df\n",
        "\n",
        "\n",
        "    '''\n",
        "    def read_tmx_files(self):\n",
        "        \"\"\"\n",
        "        Read the TMX files and return a Pandas DataFrame.\n",
        "        \"\"\"\n",
        "        segments = []\n",
        "        for file_name in self.file_names:\n",
        "            tree = ET.parse(file_name)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            for tu in root.iter('tu'):\n",
        "                source_text = \"\"\n",
        "                target_text = \"\"\n",
        "\n",
        "                for tuv in tu.iter('tuv'):\n",
        "                    lang = tuv.attrib['xml:lang']\n",
        "                    seg = tuv.find('seg').text\n",
        "\n",
        "                    if lang == 'en':\n",
        "                        source_text = seg\n",
        "                    elif lang == 'pt':\n",
        "                        target_text = seg\n",
        "\n",
        "                segments.append((source_text, target_text))\n",
        "\n",
        "        df = pd.DataFrame(segments, columns=['source', 'target'])\n",
        "        df['inner_id'] = range(1, len(df) + 1)\n",
        "        return df\n",
        "    \n",
        "    \n",
        "    def read_tmx_files(self):\n",
        "      \"\"\"\n",
        "      Read the TMX files and return a Pandas DataFrame.\n",
        "      \"\"\"\n",
        "      segments = []\n",
        "\n",
        "      for file_name in self.file_names:\n",
        "          try:\n",
        "              _, df_temp = tmx2dataframe.read(file_name)\n",
        "              segments += list(zip(df_temp['en'], df_temp['pt']))\n",
        "          except xml.parsers.expat.ExpatError as e:\n",
        "              print(f\"Error parsing {file_name}: {e}\")\n",
        "              continue\n",
        "\n",
        "      df = pd.DataFrame(segments, columns=['source', 'target'])\n",
        "      df['inner_id'] = range(1, len(df) + 1)\n",
        "      return df\n",
        "    '''\n",
        "    def delete_duplicates(self):\n",
        "        \"\"\"\n",
        "        Delete duplicates in source, target, and source+target.\n",
        "        \"\"\"\n",
        "        self.df.drop_duplicates(subset=['source'], inplace=True)\n",
        "        self.df.drop_duplicates(subset=['target'], inplace=True)\n",
        "        self.df.drop_duplicates(subset=['source', 'target'], inplace=True)\n",
        "\n",
        "    def token_count(self, text, lang):\n",
        "        \"\"\"\n",
        "        Return the number of tokens in the given text using NLTK.\n",
        "        \"\"\"\n",
        "        return len(word_tokenize(text, language=lang))\n",
        "\n",
        "    def symbol_count(self, text):\n",
        "        \"\"\"\n",
        "        Return the number of symbols in the given text.\n",
        "        \"\"\"\n",
        "        return len(text)\n",
        "\n",
        "    def sentence_count(self, text, lang):\n",
        "        \"\"\"\n",
        "        Return the number of sentences in the given text using NLTK.\n",
        "        \"\"\"\n",
        "        return len(sent_tokenize(text, language=lang))\n",
        "\n",
        "    def has_link(self, text):\n",
        "        \"\"\"\n",
        "        Check if the text contains a link using a regular expression.\n",
        "        \"\"\"\n",
        "        return bool(\n",
        "            re.search(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))\n",
        "\n",
        "    def clean_dataset(self):\n",
        "        \"\"\"\n",
        "        Clean the dataset according to the given requirements.\n",
        "        \"\"\"\n",
        "        original_length = len(self.df)\n",
        "        print(f\"Original dataset length: {original_length}\")\n",
        "        \n",
        "\n",
        "        self.delete_duplicates()\n",
        "        self.apply_filters()\n",
        "\n",
        "        self.df_filtered = self.df[\n",
        "            (self.df['source_token_count'] >= 4) & (self.df['source_token_count'] <= 200) &\n",
        "            (self.df['target_token_count'] >= 4) & (self.df['target_token_count'] <= 200) &\n",
        "            (self.df['source_symbol_count'] >= 15) & (self.df['source_symbol_count'] <= 450) &\n",
        "            (self.df['target_symbol_count'] >= 15) & (self.df['target_symbol_count'] <= 450) &\n",
        "            (self.df['source_sentence_count'] == 1) & (self.df['target_sentence_count'] == 1) &\n",
        "            (~self.df['has_link']) &\n",
        "            (~self.df['digit_mismatch']) &\n",
        "            (~self.df['invalid_target_start']) &\n",
        "            (~self.df['invalid_source_middle'])\n",
        "            ]\n",
        "        print(f\"Filtered dataset length: {len(self.df_filtered)}\")\n",
        "        filtered_length = len(self.df_filtered)\n",
        "        print(f\"Filtered dataset length: {filtered_length}\")\n",
        "        print(f\"Number of rows removed: {original_length - filtered_length}\")\n",
        "    def apply_filters(self):\n",
        "      \"\"\"\n",
        "      Apply various filters to the dataset.\n",
        "      \"\"\"\n",
        "      self.df['source_token_count'] = self.df['source'].apply(self.token_count, lang='english')\n",
        "      self.df['target_token_count'] = self.df['target'].apply(self.token_count, lang='polish')\n",
        "      self.df['source_symbol_count'] = self.df['source'].apply(self.symbol_count)\n",
        "      self.df['target_symbol_count'] = self.df['target'].apply(self.symbol_count)\n",
        "      self.df['source_sentence_count'] = self.df['source'].apply(self.sentence_count, lang='english')\n",
        "      self.df['target_sentence_count'] = self.df['target'].apply(self.sentence_count, lang='polish')\n",
        "      self.df['has_link'] = self.df['source'].apply(self.has_link) | self.df['target'].apply(self.has_link)\n",
        "\n",
        "      # Use a Pandas Series instead of DataFrame for 'digit_mismatch'\n",
        "      self.df['digit_mismatch'] = self.df.apply(\n",
        "          lambda row: set(re.findall(r'\\d+', row['source'])) != set(re.findall(r'\\d+', row['target'])), axis=1)\n",
        "\n",
        "      self.df['invalid_target_start'] = self.df['target'].str.startswith('Então')\n",
        "      self.df['invalid_source_middle'] = self.df['source'].apply(lambda x: 'actually' in x.split()[1:-1])\n",
        "\n",
        "\n",
        "    def summarize(self):\n",
        "        \"\"\"\n",
        "        Summarize the dataset by showing min and max inner_id, mean and median of source segment length in tokens and symbols.\n",
        "        \"\"\"\n",
        "        min_inner_id = self.df_filtered['inner_id'].min()\n",
        "        max_inner_id = self.df_filtered['inner_id'].max()\n",
        "        mean_source_token_count = self.df_filtered['source_token_count'].mean()\n",
        "        median_source_token_count = self.df_filtered['source_token_count'].median()\n",
        "        mean_source_symbol_count = self.df_filtered['source_symbol_count'].mean()\n",
        "        median_source_symbol_count = self.df_filtered['source_symbol_count'].median()\n",
        "\n",
        "        summary = {\n",
        "            'min_inner_id': min_inner_id,\n",
        "            'max_inner_id': max_inner_id,\n",
        "            'mean_source_token_count': mean_source_token_count,\n",
        "            'median_source_token_count': median_source_token_count,\n",
        "            'mean_source_symbol_count': mean_source_symbol_count,\n",
        "            'median_source_symbol_count': median_source_symbol_count\n",
        "        }\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def save_to_excel(self, file_name):\n",
        "        \"\"\"\n",
        "        Save the DataFrame to an Excel file.\n",
        "        \"\"\"\n",
        "        self.df_filtered.to_excel(file_name, index=False)\n",
        "    def save_to_csv(self, file_name):\n",
        "      \"\"\"\n",
        "      Save the cleaned DataFrame to a CSV file.\n",
        "      \"\"\"\n",
        "      self.df_filtered.to_csv(file_name, index=False)\n",
        "    \n",
        "    \n",
        "    def get_cleaned_dataframe(self):\n",
        "        \"\"\"\n",
        "        Return the cleaned DataFrame.\n",
        "        \"\"\"\n",
        "        return self.df_filtered\n",
        "    \n",
        "\n",
        "\"\"\"\n",
        "if __name__ == '__main__':\n",
        "    file_names = ['hr-it.tmx', 'es-is.tmx', 'en-pl.tmx']\n",
        "    cleaned_file_names = ['Cleaned_hr-it_dataset.csv', 'Cleaned_es-is_dataset.csv', 'Cleaned_en-pl_dataset.csv']\n",
        "\n",
        "    for i, file_name in enumerate(file_names):\n",
        "        dataset_cleaner = DatasetCleaner([file_name])\n",
        "        dataset_cleaner.clean_dataset()\n",
        "        dataset_cleaner.df_filtered.sort_values(by=['source_symbol_count'], ascending=True, inplace=True)\n",
        "        summary = dataset_cleaner.summarize()\n",
        "        print(f\"Summary for {file_name}: {summary}\")\n",
        "        dataset_cleaner.df_filtered.to_csv(cleaned_file_names[i], index=False)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    file_name = 'en-pl.tmx'\n",
        "    dataset_cleaner = DatasetCleaner(file_name)\n",
        "    dataset_cleaner.clean_dataset()\n",
        "    dataset_cleaner.df_filtered.sort_values(by=['source_symbol_count'], ascending=True, inplace=True)\n",
        "    summary = dataset_cleaner.summarize()\n",
        "    print(summary)\n",
        "    dataset_cleaner.save_to_csv('Cleaned_EN_PL_dataset.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw7jn4KXYaoI",
        "outputId": "6d517a99-6b6c-494c-ea4b-9fadcaaefd5a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset length: 148048\n",
            "Filtered dataset length: 109565\n",
            "Filtered dataset length: 109565\n",
            "Number of rows removed: 38483\n",
            "{'min_inner_id': 3, 'max_inner_id': 148046, 'mean_source_token_count': 18.02450600100397, 'median_source_token_count': 15.0, 'mean_source_symbol_count': 83.89812440104048, 'median_source_symbol_count': 69.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-ea35e6a77814>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset_cleaner.df_filtered.sort_values(by=['source_symbol_count'], ascending=True, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_cleaner = DatasetCleaner('en-pl.tmx')\n",
        "dataset_cleaner.clean_dataset()\n",
        "cleaned_df = dataset_cleaner.get_cleaned_dataframe()\n",
        "cleaned_df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "2ofs6jS8YywB",
        "outputId": "8cced7af-624c-4baa-b43c-42926a13aa3d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  source_language                                             source  \\\n",
              "2              en            fish,health,mission blue,oceans,science   \n",
              "4              en       Stephen Palumbi: Following the mercury trail   \n",
              "5              en     It can be a very complicated thing, the ocean.   \n",
              "6              en  And it can be a very complicated thing, what h...   \n",
              "8              en  And those simple themes aren't really themes a...   \n",
              "\n",
              "  target_language                                             target  \\\n",
              "2              pl            fish,health,mission blue,oceans,science   \n",
              "4              pl      Stephen Palumbi: Podążając rtęciowym szlakiem   \n",
              "5              pl                          Ocean jest skomplikowany.   \n",
              "6              pl                           Ludzkie zdrowie również.   \n",
              "8              pl  Nie są to ściśle naukowe rzeczy, Nie są to ści...   \n",
              "\n",
              "   inner_id  source_token_count  target_token_count  source_symbol_count  \\\n",
              "2         3                  10                  10                   39   \n",
              "4         5                   7                   6                   44   \n",
              "5         6                  11                   4                   46   \n",
              "6         7                  14                   4                   61   \n",
              "8         9                  27                  21                  131   \n",
              "\n",
              "   target_symbol_count  source_sentence_count  target_sentence_count  \\\n",
              "2                   39                      1                      1   \n",
              "4                   45                      1                      1   \n",
              "5                   25                      1                      1   \n",
              "6                   24                      1                      1   \n",
              "8                   98                      1                      1   \n",
              "\n",
              "   has_link  digit_mismatch  invalid_target_start  invalid_source_middle  \n",
              "2     False           False                 False                  False  \n",
              "4     False           False                 False                  False  \n",
              "5     False           False                 False                  False  \n",
              "6     False           False                 False                  False  \n",
              "8     False           False                 False                  False  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f3c247f9-6d08-454f-8704-1329db9b51a5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_language</th>\n",
              "      <th>source</th>\n",
              "      <th>target_language</th>\n",
              "      <th>target</th>\n",
              "      <th>inner_id</th>\n",
              "      <th>source_token_count</th>\n",
              "      <th>target_token_count</th>\n",
              "      <th>source_symbol_count</th>\n",
              "      <th>target_symbol_count</th>\n",
              "      <th>source_sentence_count</th>\n",
              "      <th>target_sentence_count</th>\n",
              "      <th>has_link</th>\n",
              "      <th>digit_mismatch</th>\n",
              "      <th>invalid_target_start</th>\n",
              "      <th>invalid_source_middle</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>en</td>\n",
              "      <td>fish,health,mission blue,oceans,science</td>\n",
              "      <td>pl</td>\n",
              "      <td>fish,health,mission blue,oceans,science</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>39</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>en</td>\n",
              "      <td>Stephen Palumbi: Following the mercury trail</td>\n",
              "      <td>pl</td>\n",
              "      <td>Stephen Palumbi: Podążając rtęciowym szlakiem</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>44</td>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>en</td>\n",
              "      <td>It can be a very complicated thing, the ocean.</td>\n",
              "      <td>pl</td>\n",
              "      <td>Ocean jest skomplikowany.</td>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>46</td>\n",
              "      <td>25</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>en</td>\n",
              "      <td>And it can be a very complicated thing, what h...</td>\n",
              "      <td>pl</td>\n",
              "      <td>Ludzkie zdrowie również.</td>\n",
              "      <td>7</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>61</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>en</td>\n",
              "      <td>And those simple themes aren't really themes a...</td>\n",
              "      <td>pl</td>\n",
              "      <td>Nie są to ściśle naukowe rzeczy, Nie są to ści...</td>\n",
              "      <td>9</td>\n",
              "      <td>27</td>\n",
              "      <td>21</td>\n",
              "      <td>131</td>\n",
              "      <td>98</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f3c247f9-6d08-454f-8704-1329db9b51a5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f3c247f9-6d08-454f-8704-1329db9b51a5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f3c247f9-6d08-454f-8704-1329db9b51a5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pd.DataFrame(cleaned_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT08PXkXcToM",
        "outputId": "62dbe6ea-8cc2-4428-e12d-62b77d5dcd90"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       source_language                                             source  \\\n",
            "2                   en            fish,health,mission blue,oceans,science   \n",
            "4                   en       Stephen Palumbi: Following the mercury trail   \n",
            "5                   en     It can be a very complicated thing, the ocean.   \n",
            "6                   en  And it can be a very complicated thing, what h...   \n",
            "8                   en  And those simple themes aren't really themes a...   \n",
            "...                ...                                                ...   \n",
            "148039              en                  I would argue life is a rheostat.   \n",
            "148042              en  Wikipedia, every day, is tens of thousands of ...   \n",
            "148043              en  It's a perfect image for the fundamental point...   \n",
            "148044              en  No one person, no one alliance, no one nation,...   \n",
            "148045              en  The vision statement of Wikipedia is very simp...   \n",
            "\n",
            "       target_language                                             target  \\\n",
            "2                   pl            fish,health,mission blue,oceans,science   \n",
            "4                   pl      Stephen Palumbi: Podążając rtęciowym szlakiem   \n",
            "5                   pl                          Ocean jest skomplikowany.   \n",
            "6                   pl                           Ludzkie zdrowie również.   \n",
            "8                   pl  Nie są to ściśle naukowe rzeczy, Nie są to ści...   \n",
            "...                ...                                                ...   \n",
            "148039              pl                   Myślę, że życie to potencjometr.   \n",
            "148042              pl  Wikipedia, to codzienny wysiłek tysięcy ludzi ...   \n",
            "148043              pl  To doskonały przykład sedna sprawy, że żaden z...   \n",
            "148044              pl  Żadna osoba, żaden sojusz, żaden kraj, nikt ni...   \n",
            "148045              pl  Motto Wikipedii jest bardzo proste: świat, w k...   \n",
            "\n",
            "        inner_id  source_token_count  target_token_count  source_symbol_count  \\\n",
            "2              3                  10                  10                   39   \n",
            "4              5                   7                   6                   44   \n",
            "5              6                  11                   4                   46   \n",
            "6              7                  14                   4                   61   \n",
            "8              9                  27                  21                  131   \n",
            "...          ...                 ...                 ...                  ...   \n",
            "148039    148040                   8                   7                   33   \n",
            "148042    148043                  24                  17                  138   \n",
            "148043    148044                  24                  19                  108   \n",
            "148044    148045                  26                  20                  103   \n",
            "148045    148046                  26                  19                  130   \n",
            "\n",
            "        target_symbol_count  source_sentence_count  target_sentence_count  \\\n",
            "2                        39                      1                      1   \n",
            "4                        45                      1                      1   \n",
            "5                        25                      1                      1   \n",
            "6                        24                      1                      1   \n",
            "8                        98                      1                      1   \n",
            "...                     ...                    ...                    ...   \n",
            "148039                   32                      1                      1   \n",
            "148042                  113                      1                      1   \n",
            "148043                   91                      1                      1   \n",
            "148044                   92                      1                      1   \n",
            "148045                  104                      1                      1   \n",
            "\n",
            "        has_link  digit_mismatch  invalid_target_start  invalid_source_middle  \n",
            "2          False           False                 False                  False  \n",
            "4          False           False                 False                  False  \n",
            "5          False           False                 False                  False  \n",
            "6          False           False                 False                  False  \n",
            "8          False           False                 False                  False  \n",
            "...          ...             ...                   ...                    ...  \n",
            "148039     False           False                 False                  False  \n",
            "148042     False           False                 False                  False  \n",
            "148043     False           False                 False                  False  \n",
            "148044     False           False                 False                  False  \n",
            "148045     False           False                 False                  False  \n",
            "\n",
            "[109565 rows x 15 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow --quiet"
      ],
      "metadata": {
        "id": "X85URJ2XhPqN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "\n",
        "# Load the cleaned data\n",
        "cleaned_data = pd.read_csv('Cleaned_EN_PL_dataset.csv')\n"
      ],
      "metadata": {
        "id": "pe4WNi8rpucQ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tokenize the source and target sentences\n",
        "source_tokenizer = Tokenizer(filters='')\n",
        "source_tokenizer.fit_on_texts(cleaned_data['source'])\n",
        "source_sequences = source_tokenizer.texts_to_sequences(cleaned_data['source'])\n",
        "\n",
        "target_tokenizer = Tokenizer(filters='')\n",
        "target_tokenizer.fit_on_texts(cleaned_data['target'])\n",
        "target_sequences = target_tokenizer.texts_to_sequences(cleaned_data['target'])\n",
        "\n",
        "# Pad the sequences\n",
        "max_source_seq_length = max([len(seq) for seq in source_sequences])\n",
        "max_target_seq_length = max([len(seq) for seq in target_sequences])\n",
        "\n",
        "source_data = pad_sequences(source_sequences, maxlen=max_source_seq_length, padding='post')\n",
        "target_data = pad_sequences(target_sequences, maxlen=max_target_seq_length, padding='post')\n"
      ],
      "metadata": {
        "id": "0KTyyE_4pwNP"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_train, source_val, target_train, target_val = train_test_split(source_data, target_data, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Bf121V88pyRp"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding"
      ],
      "metadata": {
        "id": "focdjEIGqjL3"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 256\n",
        "latent_dim = 1024\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb = Embedding(len(source_tokenizer.word_index) + 1, embedding_dim, mask_zero=True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(len(target_tokenizer.word_index) + 1, embedding_dim, mask_zero=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "\n",
        "decoder_dense = Dense(len(target_tokenizer.word_index) + 1, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "LRGHdEA-qV9i"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 30\n",
        "history = model.fit([source_train, target_train], np.expand_dims(target_train, -1),\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=([source_val, target_val], np.expand_dims(target_val, -1)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV7iRul7qYvW",
        "outputId": "34e5493e-12d3-4996-983a-c553472a1efd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "   1/1370 [..............................] - ETA: 62:49:34 - loss: 12.0265"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MjXL8sAlqoZ7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}