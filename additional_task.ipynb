{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeFtA1cPlyGdL3Eoq5aPH6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TeinkBR/intento_task/blob/master/additional_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas --quiet\n",
        "!pip install nltk --quiet\n",
        "!pip install tmx2dataframe --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8zV93PWYcZC",
        "outputId": "44f375f6-4a8b-4135-efad-04dc014b17ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tmx2dataframe (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xml --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg_SgtSqaJDQ",
        "outputId": "4ed8be8f-1412-42bb-dc80-8559dbe827a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement xml (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for xml\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "7XkTi4GTYW6e",
        "outputId": "864274d4-6519-48e4-eebf-0e49af170a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nif __name__ == \\'__main__\\':\\n    file_names = [\\'hr-it.tmx\\', \\'es-is.tmx\\', \\'en-pl.tmx\\']\\n    cleaned_file_names = [\\'Cleaned_hr-it_dataset.csv\\', \\'Cleaned_es-is_dataset.csv\\', \\'Cleaned_en-pl_dataset.csv\\']\\n\\n    for i, file_name in enumerate(file_names):\\n        dataset_cleaner = DatasetCleaner([file_name])\\n        dataset_cleaner.clean_dataset()\\n        dataset_cleaner.df_filtered.sort_values(by=[\\'source_symbol_count\\'], ascending=True, inplace=True)\\n        summary = dataset_cleaner.summarize()\\n        print(f\"Summary for {file_name}: {summary}\")\\n        dataset_cleaner.df_filtered.to_csv(cleaned_file_names[i], index=False)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from tmx2dataframe import tmx2dataframe\n",
        "import xml.parsers.expat\n",
        "class DatasetCleaner:\n",
        "    \"\"\"\n",
        "    A class for cleaning and filtering bilingual datasets stored in TMX files.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, file_names):\n",
        "        \"\"\"\n",
        "        Initialize the DatasetCleaner object with the given TMX file names.\n",
        "        \"\"\"\n",
        "        self.file_names = file_names\n",
        "        self.df = self.read_tmx_files()\n",
        "    '''\n",
        "    def read_tmx_files(self):\n",
        "        \"\"\"\n",
        "        Read the TMX files and return a Pandas DataFrame.\n",
        "        \"\"\"\n",
        "        segments = []\n",
        "        for file_name in self.file_names:\n",
        "            tree = ET.parse(file_name)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            for tu in root.iter('tu'):\n",
        "                source_text = \"\"\n",
        "                target_text = \"\"\n",
        "\n",
        "                for tuv in tu.iter('tuv'):\n",
        "                    lang = tuv.attrib['xml:lang']\n",
        "                    seg = tuv.find('seg').text\n",
        "\n",
        "                    if lang == 'en':\n",
        "                        source_text = seg\n",
        "                    elif lang == 'pt':\n",
        "                        target_text = seg\n",
        "\n",
        "                segments.append((source_text, target_text))\n",
        "\n",
        "        df = pd.DataFrame(segments, columns=['source', 'target'])\n",
        "        df['inner_id'] = range(1, len(df) + 1)\n",
        "        return df\n",
        "    '''\n",
        "\n",
        "    def read_tmx_files(self):\n",
        "      \"\"\"\n",
        "      Read the TMX files and return a Pandas DataFrame.\n",
        "      \"\"\"\n",
        "      segments = []\n",
        "\n",
        "      for file_name in self.file_names:\n",
        "          try:\n",
        "              _, df_temp = tmx2dataframe.read(file_name)\n",
        "              segments += list(zip(df_temp['en'], df_temp['pt']))\n",
        "          except xml.parsers.expat.ExpatError as e:\n",
        "              print(f\"Error parsing {file_name}: {e}\")\n",
        "              continue\n",
        "\n",
        "      df = pd.DataFrame(segments, columns=['source', 'target'])\n",
        "      df['inner_id'] = range(1, len(df) + 1)\n",
        "      return df\n",
        "\n",
        "    def delete_duplicates(self):\n",
        "        \"\"\"\n",
        "        Delete duplicates in source, target, and source+target.\n",
        "        \"\"\"\n",
        "        self.df.drop_duplicates(subset=['source'], inplace=True)\n",
        "        self.df.drop_duplicates(subset=['target'], inplace=True)\n",
        "        self.df.drop_duplicates(subset=['source', 'target'], inplace=True)\n",
        "\n",
        "    def token_count(self, text, lang):\n",
        "        \"\"\"\n",
        "        Return the number of tokens in the given text using NLTK.\n",
        "        \"\"\"\n",
        "        return len(word_tokenize(text, language=lang))\n",
        "\n",
        "    def symbol_count(self, text):\n",
        "        \"\"\"\n",
        "        Return the number of symbols in the given text.\n",
        "        \"\"\"\n",
        "        return len(text)\n",
        "\n",
        "    def sentence_count(self, text, lang):\n",
        "        \"\"\"\n",
        "        Return the number of sentences in the given text using NLTK.\n",
        "        \"\"\"\n",
        "        return len(sent_tokenize(text, language=lang))\n",
        "\n",
        "    def has_link(self, text):\n",
        "        \"\"\"\n",
        "        Check if the text contains a link using a regular expression.\n",
        "        \"\"\"\n",
        "        return bool(\n",
        "            re.search(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))\n",
        "\n",
        "    def clean_dataset(self):\n",
        "        \"\"\"\n",
        "        Clean the dataset according to the given requirements.\n",
        "        \"\"\"\n",
        "        self.delete_duplicates()\n",
        "        self.apply_filters()\n",
        "\n",
        "        self.df_filtered = self.df[\n",
        "            (self.df['source_token_count'] >= 4) & (self.df['source_token_count'] <= 200) &\n",
        "            (self.df['target_token_count'] >= 4) & (self.df['target_token_count'] <= 200) &\n",
        "            (self.df['source_symbol_count'] >= 15) & (self.df['source_symbol_count'] <= 450) &\n",
        "            (self.df['target_symbol_count'] >= 15) & (self.df['target_symbol_count'] <= 450) &\n",
        "            (self.df['source_sentence_count'] == 1) & (self.df['target_sentence_count'] == 1) &\n",
        "            (~self.df['has_link']) &\n",
        "            (~self.df['digit_mismatch']) &\n",
        "            (~self.df['invalid_target_start']) &\n",
        "            (~self.df['invalid_source_middle'])\n",
        "            ]\n",
        "\n",
        "    def apply_filters(self):\n",
        "      \"\"\"\n",
        "      Apply various filters to the dataset.\n",
        "      \"\"\"\n",
        "      self.df['source_token_count'] = self.df['source'].apply(self.token_count, lang='english')\n",
        "      self.df['target_token_count'] = self.df['target'].apply(self.token_count, lang='polish')\n",
        "      self.df['source_symbol_count'] = self.df['source'].apply(self.symbol_count)\n",
        "      self.df['target_symbol_count'] = self.df['target'].apply(self.symbol_count)\n",
        "      self.df['source_sentence_count'] = self.df['source'].apply(self.sentence_count, lang='english')\n",
        "      self.df['target_sentence_count'] = self.df['target'].apply(self.sentence_count, lang='polish')\n",
        "      self.df['has_link'] = self.df['source'].apply(self.has_link) | self.df['target'].apply(self.has_link)\n",
        "\n",
        "      # Use a Pandas Series instead of DataFrame for 'digit_mismatch'\n",
        "      self.df['digit_mismatch'] = self.df.apply(\n",
        "          lambda row: set(re.findall(r'\\d+', row['source'])) != set(re.findall(r'\\d+', row['target'])), axis=1)\n",
        "\n",
        "      self.df['invalid_target_start'] = self.df['target'].str.startswith('Então')\n",
        "      self.df['invalid_source_middle'] = self.df['source'].apply(lambda x: 'actually' in x.split()[1:-1])\n",
        "\n",
        "\n",
        "    def summarize(self):\n",
        "        \"\"\"\n",
        "        Summarize the dataset by showing min and max inner_id, mean and median of source segment length in tokens and symbols.\n",
        "        \"\"\"\n",
        "        min_inner_id = self.df_filtered['inner_id'].min()\n",
        "        max_inner_id = self.df_filtered['inner_id'].max()\n",
        "        mean_source_token_count = self.df_filtered['source_token_count'].mean()\n",
        "        median_source_token_count = self.df_filtered['source_token_count'].median()\n",
        "        mean_source_symbol_count = self.df_filtered['source_symbol_count'].mean()\n",
        "        median_source_symbol_count = self.df_filtered['source_symbol_count'].median()\n",
        "\n",
        "        summary = {\n",
        "            'min_inner_id': min_inner_id,\n",
        "            'max_inner_id': max_inner_id,\n",
        "            'mean_source_token_count': mean_source_token_count,\n",
        "            'median_source_token_count': median_source_token_count,\n",
        "            'mean_source_symbol_count': mean_source_symbol_count,\n",
        "            'median_source_symbol_count': median_source_symbol_count\n",
        "        }\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def save_to_excel(self, file_name):\n",
        "        \"\"\"\n",
        "        Save the DataFrame to an Excel file.\n",
        "        \"\"\"\n",
        "        self.df_filtered.to_excel(file_name, index=False)\n",
        "\"\"\"\n",
        "if __name__ == '__main__':\n",
        "    file_names = ['hr-it.tmx', 'es-is.tmx', 'en-pl.tmx']\n",
        "    cleaned_file_names = ['Cleaned_hr-it_dataset.csv', 'Cleaned_es-is_dataset.csv', 'Cleaned_en-pl_dataset.csv']\n",
        "\n",
        "    for i, file_name in enumerate(file_names):\n",
        "        dataset_cleaner = DatasetCleaner([file_name])\n",
        "        dataset_cleaner.clean_dataset()\n",
        "        dataset_cleaner.df_filtered.sort_values(by=['source_symbol_count'], ascending=True, inplace=True)\n",
        "        summary = dataset_cleaner.summarize()\n",
        "        print(f\"Summary for {file_name}: {summary}\")\n",
        "        dataset_cleaner.df_filtered.to_csv(cleaned_file_names[i], index=False)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    file_names = ['hr-it.tmx', 'es-is.tmx', 'en-pl.tmx']\n",
        "    cleaned_file_names = ['Cleaned_hr-it_dataset.csv', 'Cleaned_es-is_dataset.csv', 'Cleaned_en-pl_dataset.csv']\n",
        "\n",
        "    for i, file_name in enumerate(file_names):\n",
        "        dataset_cleaner = DatasetCleaner([file_name])\n",
        "        dataset_cleaner.clean_dataset()\n",
        "        dataset_cleaner.df_filtered.sort_values(by=['source_symbol_count'], ascending=True, inplace=True)\n",
        "        summary = dataset_cleaner.summarize()\n",
        "        print(f\"Summary for {file_name}: {summary}\")\n",
        "        dataset_cleaner.df_filtered.to_csv(cleaned_file_names[i], index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "Qw7jn4KXYaoI",
        "outputId": "445043b8-9cd7-4fe7-d01b-b759adbceea2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error parsing hr-it.tmx: unclosed token: line 700813, column 6\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-8f0328d52cf4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdataset_cleaner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetCleaner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdataset_cleaner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mdataset_cleaner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_filtered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source_symbol_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_cleaner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-7007d768a50b>\u001b[0m in \u001b[0;36mclean_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \"\"\"\n\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_filters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         self.df_filtered = self.df[\n",
            "\u001b[0;32m<ipython-input-12-7007d768a50b>\u001b[0m in \u001b[0;36mapply_filters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m       \u001b[0;31m# Use a Pandas Series instead of DataFrame for 'digit_mismatch'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m       self.df['digit_mismatch'] = self.df.apply(\n\u001b[0m\u001b[1;32m    134\u001b[0m           lambda row: set(re.findall(r'\\d+', row['source'])) != set(re.findall(r'\\d+', row['target'])), axis=1)\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3968\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3969\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3970\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item_frame_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3971\u001b[0m         elif (\n\u001b[1;32m   3972\u001b[0m             \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item_frame_value\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4125\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   4126\u001b[0m                 \u001b[0;34m\"Cannot set a DataFrame with multiple columns to the single \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4127\u001b[0m                 \u001b[0;34mf\"column {key}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot set a DataFrame with multiple columns to the single column digit_mismatch"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ofs6jS8YywB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}